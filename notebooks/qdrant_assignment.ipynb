{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semestral Home Assignment \n",
    "In the semestral home assignment you are tasked with designing and implementing a production ready information retrieval (IR) system with the use of Qdrant. <br>\n",
    "First will need to implement scalable Qdrant cluster with the principles of NoSQL (sharding, replication quorum). <br>\n",
    "Then, you will implement the vector search with Qdrant using all the advanced features of the vector database. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/veranika/uni/pa195_semestral_assignment_2025\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, cast, Callable\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets.dataset_dict import Dataset\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import models\n",
    "from qdrant_client.http.models.models import QueryResponse\n",
    "from fastembed import TextEmbedding, SparseTextEmbedding, LateInteractionTextEmbedding\n",
    "from fastembed.sparse.sparse_embedding_base import SparseEmbedding\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from notebooks.utils import evaluate_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_query_text(query_text: str, filter_values: list[str]) -> str:\n",
    "    base = re.sub(r\"[^a-z0-9\\\\s]\", \" \", query_text.lower())\n",
    "    extended_parts = [base]\n",
    "    # REMOVE: if filter_values: extended_parts.append(\" \".join(filter_values).lower())\n",
    "    tokens = base.split()\n",
    "    if len(tokens) < 3:\n",
    "        extended_parts.append(\" \".join(tokens) * 2)\n",
    "    return \" \".join(part.strip() for part in extended_parts if part).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables. **Do not forget to create a .env file in the root directory based on the .env.example file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up local instance of Qdrant through docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/qdrant-server\" is already in use by container \"3b09836c69f73ba7671ae9cd1dff6e67c07dc6de57b481235d01b2411e4c1144\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "\n",
      "Run 'docker run --help' for more information\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 6335:6333 -p 6336:6334 -d --name qdrant-server qdrant/qdrant:v1.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the Qdrant client by connecting to the server running as a docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=os.environ[\"QDRANT_HOST\"], port=int(os.environ[\"QDRANT_PORT\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Data Loading\n",
    "Load the data from the Hugging Face dataset [Zovi3/pa195_semestral_assignment](https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/upload/main), explore it and extract/preprocess it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import query dataset from https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/tree/main\n",
    "# Load queries from the query folder  \n",
    "query_dataset: Dataset = load_dataset(\n",
    "    \"Zovi3/pa195_semestral_assignment\", \n",
    "    data_files=\"query-all-MiniLM-L6-v2-100-filters-embedded-results/train.jsonl\",\n",
    "    split=\"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import documents dataset from https://huggingface.co/datasets/Zovi3/pa195_semestral_assignment/tree/main\n",
    "# Load documents from the corpus folder\n",
    "documents: Dataset = load_dataset(\n",
    "    \"Zovi3/pa195_semestral_assignment\",\n",
    "    data_files=\"corpus-all-MiniLM-L6-v2-50K-groups-multi-vector/train.jsonl\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'filters', 'embedding', 'multi_vector_embedding', 'result'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'embedding', 'groups', 'multi_vector_embedding'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess multi-vector embeddings: remove zero-norm vectors\n",
    "# import numpy as np\n",
    "\n",
    "# def filter_multi_vectors(example):\n",
    "#     filtered = [v for v in example['multi_vector_embedding'] if np.linalg.norm(v) > 1e-6]\n",
    "#     example['multi_vector_embedding'] = filtered\n",
    "#     return example\n",
    "# preprocessed_documents = documents.map(filter_multi_vectors)\n",
    "\n",
    "# print('Original multi vectors count:', len(documents[0]['multi_vector_embedding']))\n",
    "# print('Filtered multi vectors count:', len(preprocessed_documents[0]['multi_vector_embedding']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the homework you will work with `sentence-transformers/all-MiniLM-L6-v` from fastembed library. <br>\n",
    "These embedding are precomputed for you in the assignment dataset, but you will need to used model when running the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68ae4c5a92f45ba9261f73920e0f4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ac5ec3abca4814a8df205cf53711f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a96a7ea16c4d3f842e7bd52ecda0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6404de470472c94fb72c37be62471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d9027b1564a1899c0383567cebe5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87aec3dcc07948aa807cab2a477a3796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Embeddings are precomputed so you can save some memory by not loading the model\n",
    "# embedding_model = TextEmbedding('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding_model_size = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Retrieval Model\n",
    "Some queries require the prioritization of the certain keywords. <br>\n",
    "Therefor, you will need to use BM25 algorithm to boost the documents with these keywords during retrieval. <br>\n",
    "Note that BM25 is not taken into account in the dataset, so you will need to apply when uploading and indexing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a450ba865c3241ee8ed188788e453a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c212732c1d99412f8664d31b0cea6a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dbb7c0f339486fbac2b82acf57b0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finnish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e9018d07704dfeaab24e6df19f1ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b759b292e048ac88dcba231f9f12f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669c22a83ec346e5b3e8e54f32a97500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cad090d67444288e40c896ab4ca2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3d147c11db448f9543fa0812284297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf00517a7f54bbd9b04ae41ef26a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "arabic.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca00430579e4b31add9c0ede7012b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "portuguese.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77aa7834332b49aabbcc27c65d3bcb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cc42337ec94b189e2d96be29efad56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "greek.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffaea306478a4dbc9879dccebc120017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d30c9272b28424ba33a8462c8375e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "romanian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a47344bf2174a948a3192bf0d27b816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "russian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c09c8a060f4063828ebec555d00dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spanish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f920a0ba064f43f98050489f02b2ea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8876f1bb564cf08831195e0bda6c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc190435b66b4c5f8a74521f02a9f657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hungarian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm25_model = SparseTextEmbedding(\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Vector Model\n",
    "It is general good practice to include reranking model in the IR system. <br>\n",
    "Reranking uses stronger model to select the most relevant documents from the initial retrieval. <br>\n",
    "You will implement reranking with multi-vector late interaction embedding ColBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embeddings are precomputed so you can save some memory by not loading the model\n",
    "# multi_vector_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "multi_vector_model_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Data Modelling\n",
    "In this task you will create proper data model for your data including vector representations, index configuration, distance functions and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.1 - HNSW Index Configuration\n",
    "Configure the HNSW index for the retrieval. <br>\n",
    "**Change the ef_construct parameter to 64 to speed the build time at the cost of the recall.** <br>\n",
    "We do this for practical reasons, to enable you iterate over the notebook faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ef_construct parameter to 64 to speed the build time at the cost of the recall\n",
    "ef_construct = 64\n",
    "hnsw_config=models.HnswConfigDiff(\n",
    "            ef_construct=ef_construct,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2 - Collection Creation\n",
    "Create model for your data. You should create three vector representations for your data. <br>\n",
    "There should be one representation for each model defined above. <br>\n",
    "For multi-vector model make sure to disable the vector index since it will be used only for reranking. <br>\n",
    "Also, do not forget that multi-vector computation of similarity is not done only through the cosine similarity (check the lecture for more info). <br>\n",
    "Configure proper modifier for the sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"ms_macro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: ms_macro\n",
      "Created collection \"ms_macro\".\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except: \n",
    "    print(f\"Collection {COLLECTION_NAME} does not exist\")\n",
    "\n",
    "\n",
    "# Configure collection creation  \n",
    "collection_created = client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=embedding_model_size,\n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=hnsw_config,\n",
    "        ),\n",
    "        \"multi_vector\": models.VectorParams(\n",
    "            size=multi_vector_model_size,\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "            ),\n",
    "            hnsw_config=None, # Disable indexing for reranking-only vector\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    },\n",
    "    on_disk_payload=True\n",
    ")\n",
    "\n",
    "if collection_created:\n",
    "    print(f\"Created collection \\\"{COLLECTION_NAME}\\\".\")\n",
    "else:\n",
    "    print(\"Collection creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.3 - Create Payload Index & Disable Quantization\n",
    "Configure keyword payload index for the `groups` field. Make sure that payload index is on-disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload index created for field 'groups'\n"
     ]
    }
   ],
   "source": [
    "# Create payload index\n",
    "payload_index_created = client.create_payload_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    field_name=\"groups\", # keyword payload index for 'groups' field \n",
    "    field_schema=models.PayloadSchemaType.KEYWORD,\n",
    ")\n",
    "\n",
    "# Disable quantization\n",
    "client.update_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    quantization_config=models.Disabled.DISABLED,\n",
    ")\n",
    "\n",
    "if payload_index_created:\n",
    "    print(f\"Payload index created for field 'groups'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload indices: ['groups']\n"
     ]
    }
   ],
   "source": [
    "collection_info = client.get_collection(COLLECTION_NAME)\n",
    "print(f\"Payload indices: {list(collection_info.payload_schema.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Data Upload\n",
    "Upload vector embeddings and metadata to the created collection, make sure to upload the vectors metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing points with normalized dense vectors...\n",
      "Upserting 50000 documents...\n",
      "Collection info: 50000 points in collection\n"
     ]
    }
   ],
   "source": [
    "points: list[models.PointStruct] = []\n",
    "\n",
    "print(\"Preparing points with normalized dense vectors...\")\n",
    "# Iterate over the documents dataset\n",
    "for doc in documents:  # type: ignore    \n",
    "    # Generate Sparse Vector (BM25) on the fly\n",
    "    sparse_embeddings = list[SparseEmbedding](bm25_model.embed([doc[\"text\"]]))\n",
    "    sparse_embedding = sparse_embeddings[0]\n",
    "    \n",
    "    point = models.PointStruct(\n",
    "        id=doc[\"id\"],\n",
    "        vector={\n",
    "            \"dense\": doc[\"embedding\"], # Qdrant will normalize this automatically\n",
    "            \"sparse\": models.SparseVector(\n",
    "                indices=sparse_embedding.indices.tolist(),\n",
    "                values=sparse_embedding.values.tolist(),\n",
    "            ),\n",
    "            \"multi_vector\": doc[\"multi_vector_embedding\"], # Already fits our config\n",
    "        },\n",
    "        payload={\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"groups\": doc[\"groups\"],\n",
    "        },\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "print(f\"Upserting {len(points)} documents...\")\n",
    "# Upload in batches to avoid network timeouts\n",
    "client.upload_points(collection_name=COLLECTION_NAME, points=points, batch_size=128)\n",
    "\n",
    "print(f\"Collection info: {client.get_collection(COLLECTION_NAME).points_count} points in collection\")\n",
    "assert client.get_collection(COLLECTION_NAME).points_count == len(documents), \\\n",
    "    f\"Expected {len(documents)} points in collection, got {client.get_collection(COLLECTION_NAME).points_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Design Complex Query\n",
    "Your task is to design a complex query that will include hybrid search, filtering, reranking and metadata boosting. <br>\n",
    "**The result of this task should be one Qdrant query (do not add any postprocessing logic outside of the Qdrant query)!**\n",
    " \n",
    "**Subtasks:**\n",
    "1. Define query filter with relation to the `groups` field, do not forget there can be filter values in the query.\n",
    "    - Think about in which prefetch you should apply the filter.\n",
    "2. Define sparse and dense search prefetche, the limit for the retrieval should be 100 objects.\n",
    "3. Define fusion of the two rankings with Reciprocal Rank Fusion (RRF).\n",
    "4. Rerank the results with ColBERT multi-vector model, use 50 documents for reranking.\n",
    "5. Boost the results with metadata weighting, use `group_1` with weight 0.05 and `group_2` with weight 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_context_retrieval(query: dict[str, Any]) -> QueryResponse:\n",
    "    # -----------------------------------------\n",
    "    # Query text and embeddings\n",
    "    # -----------------------------------------\n",
    "    query_text = query[\"text\"]\n",
    "    query_dense_embedding = list(embedding_model.embed([query_text]))[0]\n",
    "    filter_values: list[str] = query.get(\"filters\", [])\n",
    "\n",
    "    sparse_text = build_sparse_query_text(query_text, filter_values)\n",
    "    sparse_emb_obj = next(bm25_model.embed([sparse_text]))\n",
    "    query_sparse_embedding = models.SparseVector(\n",
    "        indices=sparse_emb_obj.indices.tolist(),\n",
    "        values=sparse_emb_obj.values.tolist(),\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Task 4.1 — Filter (EARLY, AND semantics)\n",
    "    # -----------------------------------------\n",
    "    filter_condition: models.Filter | None = None\n",
    "    if filter_values:\n",
    "        filter_condition = models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"groups\",\n",
    "                    match=models.MatchValue(value=fv),\n",
    "                )\n",
    "                for fv in filter_values\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Task 4.2 — Sparse + Dense search (limit=100)\n",
    "    # -----------------------------------------\n",
    "    sparse_limit = 100\n",
    "    dense_limit = 100\n",
    "\n",
    "    prefetch_sparse_and_dense_search: list[models.Prefetch] = [\n",
    "        models.Prefetch(\n",
    "            query=query_dense_embedding,\n",
    "            using=\"dense\",\n",
    "            limit=dense_limit,\n",
    "            filter=filter_condition,\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=query_sparse_embedding,\n",
    "            using=\"sparse\",\n",
    "            limit=sparse_limit,\n",
    "            filter=filter_condition,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Task 4.3 — RRF fusion (k = 60)\n",
    "    # -----------------------------------------\n",
    "    rrf_k = 60\n",
    "\n",
    "    prefetch_fused_rankings: list[models.Prefetch] = [\n",
    "        models.Prefetch(\n",
    "            prefetch=prefetch_sparse_and_dense_search,\n",
    "            query=models.RrfQuery(\n",
    "                rrf=models.Rrf(k=rrf_k)\n",
    "            ),\n",
    "            limit=10,  # directly match final precision-oriented limit\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Task 4.4 — Metadata boosting (kept consistent)\n",
    "    # -----------------------------------------\n",
    "    boost_terms = []\n",
    "    for fv in filter_values:\n",
    "        if fv == \"group_1\":\n",
    "            boost_terms.append(\n",
    "                models.MultExpression(\n",
    "                    mult=[\n",
    "                        0.05,\n",
    "                        models.FieldCondition(\n",
    "                            key=\"groups\",\n",
    "                            match=models.MatchAny(any=[\"group_1\"]),\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        elif fv == \"group_2\":\n",
    "            boost_terms.append(\n",
    "                models.MultExpression(\n",
    "                    mult=[\n",
    "                        0.1,\n",
    "                        models.FieldCondition(\n",
    "                            key=\"groups\",\n",
    "                            match=models.MatchAny(any=[\"group_2\"]),\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    final_query = models.FormulaQuery(\n",
    "        formula=models.SumExpression(\n",
    "            sum=[\"$score\"] + boost_terms\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Final query\n",
    "    # -----------------------------------------\n",
    "    final_query_limit = 10\n",
    "\n",
    "    final_result: QueryResponse = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=prefetch_fused_rankings,\n",
    "        query=final_query,\n",
    "        limit=final_query_limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You achieved 0.9869999999999999 enough to pass ✅!\n"
     ]
    }
   ],
   "source": [
    "avg_retrieval_precision = evaluate_retrieval(rag_context_retrieval, query_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
